{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g_Qp173_NbG5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-09 19:07:53.361419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-09 19:07:54.058132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import IPython.display as display\n",
        "import PIL.Image\n",
        "from scipy.io import wavfile\n",
        "from scipy.io.wavfile import write\n",
        "import cv2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wgeIJg82NbG4"
      },
      "source": [
        "## Choose an image to dream-ify"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yt6zam_9NbG4"
      },
      "source": [
        "For this tutorial, let's use an image of a [labrador](https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Y5BPgc8NNbG0"
      },
      "outputs": [],
      "source": [
        "# Download an image and read it into a NumPy array.\n",
        "def load(filePath, segment):\n",
        "  # read the stereo audio file\n",
        "  _, a = wavfile.read(filePath)\n",
        "  stereo_signal = np.array(a, dtype=np.float32)\n",
        "\n",
        "  # split the stereo signal into left and right channels\n",
        "  stereo_signal[:, 0] = stereo_signal[:, 0] / 32768.0\n",
        "  stereo_signal[:, 1] = stereo_signal[:, 1] / 32768.0\n",
        "  whole_left = stereo_signal[:, 0][segment * 309000 : ((segment + 1) * 309000)]\n",
        "  whole_right = stereo_signal[:, 0][segment * 309000 : ((segment + 1) * 309000)]\n",
        "  left_channel = whole_left[:103000]\n",
        "  right_channel = whole_right[:103000]\n",
        "\n",
        "  left_2 = whole_left[103000:206000]\n",
        "  right_2 = whole_right[103000:206000]\n",
        "\n",
        "  left_3 = whole_left[206000:309000]\n",
        "  right_3 = whole_right[206000:309000]\n",
        "\n",
        "  res1 = np.vstack((left_channel, right_channel))\n",
        "  res1 = np.reshape(res1, (412, 500))\n",
        "  res2 = np.vstack((left_2, right_2))\n",
        "  res2 = np.reshape(res2, (412, 500))\n",
        "  res3 = np.vstack((left_3, right_3))\n",
        "  res3 = np.reshape(res3, (412, 500))\n",
        "  fin = cv2.merge((res1,res2,res3))\n",
        "  return fin\n",
        "\n",
        "# Normalize an image\n",
        "def deprocess(img):\n",
        "  img = 255*(img + 1.0)/2.0\n",
        "  return tf.cast(img, tf.uint8)\n",
        "\n",
        "# Display an image\n",
        "def show(img, seq):\n",
        "  pilobj = PIL.Image.fromarray(np.array(img))\n",
        "  display.display(pilobj)\n",
        "  pilobj.save(str(seq) + \".jpg\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F4RBFfIWNbG0"
      },
      "source": [
        "## Prepare the feature extraction model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cruNQmMDNbGz"
      },
      "source": [
        "Download and prepare a pre-trained image classification model. You will use [InceptionV3](https://keras.io/api/applications/inceptionv3/) which is similar to the model originally used in DeepDream. Note that any [pre-trained model](https://keras.io/api/applications/#available-models) will work, although you will have to adjust the layer names below if you change this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlLi48GKNbGy",
        "outputId": "09547a80-c87b-46ea-e8fb-97d02e7ddb45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-09 19:07:54.912769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-09 19:07:54.937026: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bujb0jPNNbGx"
      },
      "source": [
        "The idea in DeepDream is to choose a layer (or layers) and maximize the \"loss\" in a way that the image increasingly \"excites\" the layers. The complexity of the features incorporated depends on layers chosen by you, i.e, lower layers produce strokes or simple patterns, while deeper layers give sophisticated features in images, or even whole objects."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qOVmDO4LNbGv"
      },
      "source": [
        "The InceptionV3 architecture is quite large (for a graph of the model architecture see TensorFlow's [research repo](https://github.com/tensorflow/models/tree/master/research/slim)). For DeepDream, the layers of  interest are those where the convolutions are concatenated. There are 11 of these layers in InceptionV3, named 'mixed0' though 'mixed10'. Using different layers will result in different dream-like images. Deeper layers respond to higher-level features (such as eyes and faces), while earlier layers respond to simpler features (such as edges, shapes, and textures). Feel free to experiment with the layers selected below, but keep in mind that deeper layers (those with a higher index) will take longer to train on since the gradient computation is deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "08KB502ONbGt"
      },
      "outputs": [],
      "source": [
        "# Maximize the activations of these layers\n",
        "names = ['mixed3', 'mixed5']\n",
        "layers = [base_model.get_layer(name).output for name in names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sb7u31B4NbGt"
      },
      "source": [
        "## Calculate loss\n",
        "\n",
        "The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. Normally, loss is a quantity you wish to minimize via gradient descent. In DeepDream, you will maximize this loss via gradient ascent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8MhfSweXXiuq"
      },
      "outputs": [],
      "source": [
        "def calc_loss(img, model):\n",
        "  # Pass forward the image through the model to retrieve the activations.\n",
        "  # Converts the image into a batch of size 1.\n",
        "  img_batch = tf.expand_dims(img, axis=0)\n",
        "  layer_activations = model(img_batch)\n",
        "  if len(layer_activations) == 1:\n",
        "    layer_activations = [layer_activations]\n",
        "\n",
        "  losses = []\n",
        "  for act in layer_activations:\n",
        "    loss = tf.math.reduce_mean(act)\n",
        "    losses.append(loss)\n",
        "\n",
        "  return  tf.reduce_sum(losses)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k4TCNsAUO9kI"
      },
      "source": [
        "## Gradient ascent\n",
        "\n",
        "Once you have calculated the loss for the chosen layers, all that is left is to calculate the gradients with respect to the image, and add them to the original image. \n",
        "\n",
        "Adding the gradients to the image enhances the patterns seen by the network. At each step, you will have created an image that increasingly excites the activations of certain layers in the network.\n",
        "\n",
        "The method that does this, below, is wrapped in a `tf.function` for performance. It uses an `input_signature` to ensure that the function is not retraced for different image sizes or `steps`/`step_size` values. See the [Concrete functions guide](../../guide/function.ipynb) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qRScWg_VNqvj"
      },
      "outputs": [],
      "source": [
        "class DeepDream(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "      input_signature=(\n",
        "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=[], dtype=tf.int32),\n",
        "        tf.TensorSpec(shape=[], dtype=tf.float32),)\n",
        "  )\n",
        "  def __call__(self, img, steps, step_size):\n",
        "      print(\"Tracing\")\n",
        "      loss = tf.constant(0.0)\n",
        "      for n in tf.range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "          # This needs gradients relative to `img`\n",
        "          # `GradientTape` only watches `tf.Variable`s by default\n",
        "          tape.watch(img)\n",
        "          loss = calc_loss(img, self.model)\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
        "        gradients = tape.gradient(loss, img)\n",
        "\n",
        "        # Normalize the gradients.\n",
        "        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "        print(n, steps, gradients)\n",
        "        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
        "        # You can update the image by directly adding the gradients (because they're the same shape!)\n",
        "        img = img + gradients*step_size\n",
        "        # img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "      return loss, img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yB9pTqn6xfuK"
      },
      "outputs": [],
      "source": [
        "deepdream = DeepDream(dream_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XLArRTVHZFAi"
      },
      "source": [
        "## Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9vHEcy7dTysi"
      },
      "outputs": [],
      "source": [
        "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n",
        "  # Convert from uint8 to the range expected by the model.\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  img = tf.convert_to_tensor(img)\n",
        "  step_size = tf.convert_to_tensor(step_size)\n",
        "  steps_remaining = steps\n",
        "  step = 0\n",
        "  while steps_remaining:\n",
        "    if steps_remaining>100:\n",
        "      run_steps = tf.constant(100)\n",
        "    else:\n",
        "      run_steps = tf.constant(steps_remaining)\n",
        "    steps_remaining -= run_steps\n",
        "    step += run_steps\n",
        "  loss, img = deepdream(img, steps, tf.constant(step_size))\n",
        "  vis = tf.clip_by_value(img, -1, 1)\n",
        "  display.clear_output(wait=True)\n",
        "  print (\"Step {}, loss {}\".format(step, loss))\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tEfd00rr0j8Z",
        "outputId": "0e89b53e-304e-44e8-8c04-7808189d73e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1, loss 0.3715117573738098\n"
          ]
        }
      ],
      "source": [
        "# Downsizing the image makes it easier to work with.\n",
        "fin = np.zeros((2, 0))\n",
        "samplingRate = 48000\n",
        "filePath = \"rickroll.wav\"\n",
        "name = filePath.split(\".\")[0]\n",
        "_, stereo_signal = wavfile.read(filePath)\n",
        "iter = int(len(stereo_signal[:, 0]) / 309000) \n",
        "for i in range(0, iter):\n",
        "  original_img = load(filePath, segment=i)\n",
        "  show(deprocess(original_img), i)\n",
        "  dream_img = run_deep_dream_simple(img=original_img, steps=1, step_size=1e-11)\n",
        "  dream_img = dream_img.numpy()\n",
        "  dream_img = dream_img.flatten()\n",
        "  dream_img = np.reshape(dream_img, (2, 309000))\n",
        "  fin = np.hstack((fin, dream_img))\n",
        "fin = (fin * 32767).astype(np.int16)\n",
        "write(name + \"-output.wav\", samplingRate, fin.T)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Post processing\n",
        "\n",
        "Need to cut down the amplitude/volume because it always destroys my ears if I don't clean it up in Audacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wave, audioop\n",
        "factor = 0.125\n",
        "with wave.open(name + '-output.wav', 'rb') as wav:\n",
        "    p = wav.getparams()\n",
        "    with wave.open(name + '-output-corrected.wav', 'wb') as audio:\n",
        "        audio.setparams(p)\n",
        "        frames = wav.readframes(p.nframes)\n",
        "        audio.writeframesraw(audioop.mul(frames, p.sampwidth, factor))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
